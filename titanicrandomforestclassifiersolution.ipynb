{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/robertwalmsley/titanicvotingclassifiersolution?scriptVersionId=146272663\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"3f6b6aff","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-12T13:43:02.698586Z","iopub.status.busy":"2023-10-12T13:43:02.698126Z","iopub.status.idle":"2023-10-12T13:43:02.712495Z","shell.execute_reply":"2023-10-12T13:43:02.710794Z"},"papermill":{"duration":0.022993,"end_time":"2023-10-12T13:43:02.715534","exception":false,"start_time":"2023-10-12T13:43:02.692541","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/titanic/train.csv\n","/kaggle/input/titanic/test.csv\n","/kaggle/input/titanic/gender_submission.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"id":"86020771","metadata":{"execution":{"iopub.execute_input":"2023-10-12T13:43:02.722164Z","iopub.status.busy":"2023-10-12T13:43:02.721768Z","iopub.status.idle":"2023-10-12T13:43:13.402723Z","shell.execute_reply":"2023-10-12T13:43:13.400855Z"},"papermill":{"duration":10.687512,"end_time":"2023-10-12T13:43:13.405402","exception":false,"start_time":"2023-10-12T13:43:02.71789","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Best parameters for Boosted forest:  {'n_estimators': 200}\n","Best parameters for SVM:  {'C': 1}\n","Best parameters for logistic regression:  {'C': 0.1}\n","Accuracy:  0.8156424581005587\n","Best parameters for voting classifier:  {'weights': [1, 1, 2]}\n","Accuracy again:  0.7988826815642458\n","Accuracy of logistic regression classifier:  0.7964148527528808\n","Accuracy of Boosted forest classifier:  0.8203092681965922\n","Accuracy of SVM classifier:  0.8385994287402738\n"]}],"source":["import pandas as pd\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","\n","# Read in the training and test sets\n","train_df = pd.read_csv('/kaggle/input/titanic/train.csv')\n","test_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n","\n","# Preprocess the data\n","\n","# Identify most relevant features\n","# You can use techniques like feature importance or correlation analysis to help you identify the most important features\n","relevant_features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n","\n","# Handle missing values\n","imputer = SimpleImputer(strategy='most_frequent')\n","train_df[relevant_features] = imputer.fit_transform(train_df[relevant_features])\n","test_df[relevant_features] = imputer.transform(test_df[relevant_features])\n","\n","# Encode categorical variables as numeric\n","train_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})\n","test_df['Sex'] = test_df['Sex'].map({'male': 0, 'female': 1})\n","train_df['Embarked'] = train_df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n","test_df['Embarked'] = test_df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n","\n","# Transform skewed or non-normal features\n","# Instead of normalizing all of the numeric features, you could try using techniques like log transformation or Box-Cox transformation to make the distribution of a feature more normal\n","scaler = StandardScaler()\n","train_df[relevant_features] = scaler.fit_transform(train_df[relevant_features])\n","test_df[relevant_features] = scaler.transform(test_df[relevant_features])\n","\n","# Split the data into features (X) and labels (y)\n","X_train = train_df[relevant_features]\n","y_train = train_df['Survived']\n","X_test = test_df[relevant_features]\n","\n","# Split the data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n","\n","# Train the model\n","log_reg = LogisticRegression()\n","bTree = GradientBoostingClassifier(min_samples_split=10)\n","svm = SVC()\n","model = VotingClassifier(estimators=[('lr', log_reg), ('dt', bTree), ('svm', svm)])\n","model.fit(X_train, y_train)\n","\n","\n","# Fine-tune the model\n","param_grid = {'n_estimators': [50, 100, 200]}\n","grid_search = GridSearchCV(bTree, param_grid, cv=5)\n","grid_search.fit(X_train, y_train)\n","best_bTree = grid_search.best_estimator_\n","print(\"Best parameters for Boosted forest: \", grid_search.best_params_)\n","\n","param_grid = {'C': [0.1, 1, 10]}\n","grid_search = GridSearchCV(svm, param_grid, cv=5)\n","grid_search.fit(X_train, y_train)\n","best_svm = grid_search.best_estimator_\n","print(\"Best parameters for SVM: \", grid_search.best_params_)\n","\n","param_grid = {'C': [0.1, 1, 10]}\n","grid_search = GridSearchCV(log_reg, param_grid, cv=5)\n","grid_search.fit(X_train, y_train)\n","best_log_reg = grid_search.best_estimator_\n","print(\"Best parameters for logistic regression: \", grid_search.best_params_)\n","\n","# Retrain the model with the best individual parameters\n","model = VotingClassifier(estimators=[('lr', best_log_reg), ('dt', best_bTree), ('svm', best_svm)])\n","model.fit(X_train, y_train)\n","\n","# Evaluate the fine-tuned model\n","y_pred = model.predict(X_val)\n","accuracy = accuracy_score(y_val, y_pred)\n","print(\"Accuracy: \", accuracy)\n","\n","# Fine-tune the voting classifier model using grid search\n","param_grid = {'weights': [[1, 1, 1], [2, 1, 1], [1, 2, 1], [1, 1, 2], [2, 2, 1], [2, 1, 2], [1, 2, 2], [2, 2, 2]]}\n","model = VotingClassifier(estimators=[('lr', log_reg), ('dt', bTree), ('svm', svm)])\n","model.fit(X_train, y_train)\n","grid_search = GridSearchCV(model, param_grid, cv=5)\n","grid_search.fit(X_train, y_train)\n","best_model = grid_search.best_estimator_\n","print(\"Best parameters for voting classifier: \", grid_search.best_params_)\n","\n","# Evaluate the fine-tuned model\n","y_pred = best_model.predict(X_val)\n","accuracy = accuracy_score(y_val, y_pred)\n","print(\"Accuracy again: \", accuracy)\n","\n","# Evaluate the logistic regression classifier\n","scores = cross_val_score(log_reg, X_train, y_train, cv=5)\n","print(\"Accuracy of logistic regression classifier: \", scores.mean())\n","\n","# Evaluate the bTree classifier\n","scores = cross_val_score(bTree, X_train, y_train, cv=5)\n","print(\"Accuracy of Boosted forest classifier: \", scores.mean())\n","\n","# Evaluate the SVM classifier\n","scores = cross_val_score(svm, X_train, y_train, cv=5)\n","print(\"Accuracy of SVM classifier: \", scores.mean())\n","\n","\n","# Make predictions on the test set\n","y_pred = best_model.predict(X_test)\n","\n","# Save the predictions to a CSV file\n","output = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': y_pred})\n","output.to_csv('submission.csv', index=False)\n"]},{"cell_type":"markdown","id":"3460eeba","metadata":{"papermill":{"duration":0.002853,"end_time":"2023-10-12T13:43:13.411184","exception":false,"start_time":"2023-10-12T13:43:13.408331","status":"completed"},"tags":[]},"source":["Accuracy:  0.8435754189944135                                 \n","Best parameters for random forest:  {'n_estimators': 200}                                        \n","Best parameters for SVM:  {'C': 1}                                        \n","Best parameters for logistic regression:  {'C': 0.1}                                        "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":24.957319,"end_time":"2023-10-12T13:43:16.304899","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-10-12T13:42:51.34758","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}